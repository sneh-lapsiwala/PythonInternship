{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Elevate Labs AI & ML Internship - Task 1: Data Cleaning & Preprocessing\n",
        "\n",
        "**Objective:** Learn how to clean and prepare raw data for machine learning (ML) models using robust and reproducible techniques. This notebook follows the \"Hints/Mini Guide\" provided in the task document, demonstrating each step with the Titanic dataset.\n",
        "\n",
        "**Tools Used:** Python, Pandas, NumPy, Matplotlib/Seaborn, Scikit-learn (sklearn)"
      ],
      "metadata": {
        "id": "BskDLzVn_lBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1: Setup and Library Imports\n",
        "This cell imports all the necessary Python libraries for data manipulation, visualization, and machine learning preprocessing."
      ],
      "metadata": {
        "id": "hibjT67HAHxO"
      }
    },
    {
      "source": [
        "# @title Setup and Library Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All necessary libraries imported successfully!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wV8sOslz_Amp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2: Load the Dataset & Initial Exploration\n",
        "We will load the Titanic dataset directly from a raw GitHub URL, which is a convenient method for Google Colab. After loading, we perform initial data exploration to understand its structure, data types, and identify missing values.\n",
        "\n",
        "This directly addresses **Hint/Mini Guide Point 1: \"Import the dataset and explore basic info (nulls, data types).\"**"
      ],
      "metadata": {
        "id": "bwZXESBEAw20"
      }
    },
    {
      "source": [
        "# @title Load the Dataset & Initial Exploration\n",
        "# URL for the raw Titanic train dataset on GitHub\n",
        "titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(titanic_url)\n",
        "    print(\"Titanic Dataset loaded successfully from URL.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure your internet connection is stable or check the URL.\")\n",
        "\n",
        "print(\"\\n--- Original Dataset Information (df.info()) ---\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n--- Missing Values Count (df.isnull().sum()) ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- First 5 Rows of the Dataset (df.head()) ---\")\n",
        "print(df.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "3rvNjFHD_Df8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Initial Exploration:**\n",
        "- `df.info()` reveals that 'Age', 'Cabin', and 'Embarked' columns have missing values (fewer non-null entries than total entries).\n",
        "- `df.isnull().sum()` confirms the exact number of missing values: 'Age' (177), 'Cabin' (687), 'Embarked' (2).\n",
        "- Data types are mostly appropriate, with 'object' types indicating categorical features like 'Name', 'Sex', 'Ticket', 'Cabin', and 'Embarked'."
      ],
      "metadata": {
        "id": "lmnTK64HCEDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 3: EDA**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jN5e62TkEUIm"
      }
    },
    {
      "source": [
        "# @title üîç EDA: Categorical Feature Counts, Survival Rate & Age Distribution\n",
        "\n",
        "print(\"\\n--- Value Counts for Key Categorical Features ---\")\n",
        "print(\"Sex:\\n\", df['Sex'].value_counts())\n",
        "print(\"\\nEmbarked:\\n\", df['Embarked'].value_counts())\n",
        "print(\"\\nPclass (Passenger Class):\\n\", df['Pclass'].value_counts())\n",
        "\n",
        "print(\"\\n--- Survival Rate Distribution ---\")\n",
        "print(df['Survived'].value_counts(normalize=True))\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.countplot(x='Survived', data=df)\n",
        "plt.title('Survival Count (0=No, 1=Yes)')\n",
        "plt.xlabel('Survived')\n",
        "plt.ylabel('Number of Passengers')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['Age'].dropna(), bins=30, kde=True, color='skyblue')\n",
        "plt.title('Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "QI0G2tcC_J8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of EDA:**\n",
        "- The dataset is imbalanced towards non-survivors (around 61.6% did not survive).\n",
        "- Age distribution is somewhat normal, but with a tail towards older ages and a peak for young adults.\n",
        "- 'Sex', 'Embarked', and 'Pclass' are important categorical features influencing survival."
      ],
      "metadata": {
        "id": "f0ox7vo8DTxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Define Preprocessing Steps using Pipelines and ColumnTransformer\n",
        "This is the core of our enhanced preprocessing. We define a robust pipeline using sklearn's Pipeline and ColumnTransformer. This approach ensures:\n",
        "\n",
        "*   **Consistency:** The same transformations are applied to both training and test data.\n",
        "*   **No Data Leakage:** `fit` operations (like learning medians for imputation or scaling parameters) are only performed on the training data.\n",
        "*   **Modularity:** Steps are clearly defined and can be easily modified or extended.\n",
        "\n",
        "This section directly addresses:\n",
        "\n",
        "*   **Hint/Mini Guide Point 2:** \"Handle missing values using mean/median/imputation.\"\n",
        "*   **Hint/Mini Guide Point 3:** \"Convert categorical features into numerical using encoding.\"\n",
        "*   **Hint/Mini Guide Point 4:** \"Normalize/standardize the numerical features.\"\n",
        "*   **Hint/Mini Guide Point 5 (partially):** \"Visualize outliers and remove them.\" - We implement a custom `OutlierCapper` to \"remove\" (by capping) outliers within the pipeline."
      ],
      "metadata": {
        "id": "v--1PPVLDt_K"
      }
    },
    {
      "source": [
        "# @title Define Preprocessing Steps using Pipelines and ColumnTransformer\n",
        "# --- Define Features for Preprocessing ---\n",
        "numerical_features = ['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']\n",
        "categorical_features = ['Sex', 'Embarked']\n",
        "\n",
        "# --- Custom Transformer for Outlier Capping ---\n",
        "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, lower_percentile=1, upper_percentile=99):\n",
        "        self.lower_percentile = lower_percentile\n",
        "        self.upper_percentile = upper_percentile\n",
        "        self.lower_bounds = {}\n",
        "        self.upper_bounds = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for col in X.columns:\n",
        "            if pd.api.types.is_numeric_dtype(X[col]):\n",
        "                self.lower_bounds[col] = np.percentile(X[col].dropna(), self.lower_percentile)\n",
        "                self.upper_bounds[col] = np.percentile(X[col].dropna(), self.upper_percentile)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "        for col in X.columns:\n",
        "            if col in self.lower_bounds and col in self.upper_bounds:\n",
        "                X_transformed[col] = np.clip(X_transformed[col], self.lower_bounds[col], self.upper_bounds[col])\n",
        "        return X_transformed\n",
        "\n",
        "print(\"Custom OutlierCapper transformer defined for integration into the pipeline.\")\n",
        "\n",
        "# --- 1. Preprocessing Pipeline for Numerical Features ---\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('outlier_capper', OutlierCapper(lower_percentile=1, upper_percentile=99)),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# --- 2. Preprocessing Pipeline for Categorical Features ---\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
        "])\n",
        "\n",
        "# --- 3. Combine Preprocessing Steps using ColumnTransformer ---\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# --- 4. Create a Full Machine Learning Pipeline ---\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', random_state=42))\n",
        "])\n",
        "\n",
        "print(\"\\nPreprocessing pipeline defined successfully, integrating imputation, outlier capping, scaling, and encoding.\")\n",
        "print(\"The full pipeline is ready to be fitted to your data:\")\n",
        "print(model_pipeline)"
      ],
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Qt1edbyl_Pfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 5: Prepare Data for Training (Train-Test Split)\n",
        "\n",
        "It is crucial to split the data into training and testing sets before applying any data transformations. This prevents \"data leakage,\" where information from the test set inadvertently influences the training process, leading to an overly optimistic evaluation of model performance."
      ],
      "metadata": {
        "id": "P9qMI3xzHrJZ"
      }
    },
    {
      "source": [
        "# @title Prepare Data for Training (Train-Test Split)\n",
        "# Separate features (X) from the target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"Training features (X_train) shape: {X_train.shape}\")\n",
        "print(f\"Testing features (X_test) shape: {X_test.shape}\")\n",
        "print(f\"Training target (y_train) shape: {y_train.shape}\")\n",
        "print(f\"Testing target (y_test) shape: {y_test.shape}\")\n",
        "\n",
        "print(\"\\nData successfully split into training and testing sets, preventing data leakage.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pHDopGGH_Sct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 6: Train the Pipeline and Evaluate the Model\n",
        "\n",
        "Now, we fit the entire `model_pipeline` to the training data. The pipeline will automatically execute all defined preprocessing steps (imputation, outlier capping, scaling, encoding) on `X_train`, and then train the Logistic Regression classifier on the transformed data. Finally, we evaluate the model's performance on both the training and unseen test sets."
      ],
      "metadata": {
        "id": "LZ4LXELjKJvl"
      }
    },
    {
      "source": [
        "# @title Train the Pipeline and Evaluate the Model\n",
        "print(\"Fitting the complete preprocessing and classification pipeline to the training data...\")\n",
        "model_pipeline.fit(X_train, y_train)\n",
        "print(\"Pipeline fitting complete! The model has learned from the training data.\")\n",
        "\n",
        "train_accuracy = model_pipeline.score(X_train, y_train)\n",
        "test_accuracy = model_pipeline.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "y_pred = model_pipeline.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(\"\\n--- Confusion Matrix (on Test Data) ---\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\n--- Classification Report (on Test Data) ---\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GCtO2IQJ_Vfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Model Evaluation:**\n",
        "\n",
        "- The model achieved a training accuracy of (Accuracy value) and a test accuracy of (Accuracy value).\n",
        "- The confusion matrix and classification report provide deeper insights into the model's performance for both 'Survived' (1) and 'Not Survived' (0) classes."
      ],
      "metadata": {
        "id": "PWT5uyjGKUmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 7: Visualize Outliers (Before & After Preprocessing)\n",
        "\n",
        "This cell demonstrates the effect of our `OutlierCapper` and `StandardScaler` by visualizing the numerical features before and after they pass through the preprocessing pipeline.\n",
        "\n",
        "This directly addresses **Hint/Mini Guide Point 5: \"Visualize outliers...\"** and shows the effect of \"remove them\" via capping."
      ],
      "metadata": {
        "id": "cIDjbdC5KZgY"
      }
    },
    {
      "source": [
        "# @title Visualize Outliers (Before & After Preprocessing)\n",
        "# --- Visualizing Outliers in Original Numerical Features ---\n",
        "print(\"\\n--- Boxplots of Original Numerical Features (Before Preprocessing) ---\")\n",
        "outlier_features_for_viz = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i, col in enumerate(outlier_features_for_viz):\n",
        "    plt.subplot(1, len(outlier_features_for_viz), i + 1)\n",
        "    sns.boxplot(y=df[col].dropna(), palette='viridis')\n",
        "    plt.title(f'Original {col}')\n",
        "    plt.ylabel(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Q3udpA1l_aDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:** These boxplots show the presence of outliers in features like 'Age', 'Fare', 'SibSp', and 'Parch' in the raw dataset."
      ],
      "metadata": {
        "id": "cBdUJe6QKvg1"
      }
    },
    {
      "source": [
        "# @title Visualizing Numerical Features After Preprocessing\n",
        "# --- Visualizing Numerical Features After Preprocessing (Capping & Scaling) ---\n",
        "print(\"\\n--- Boxplots of Numerical Features (After Capping & Standardization in Pipeline) ---\")\n",
        "\n",
        "fitted_numerical_transformer = model_pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
        "\n",
        "X_train_numerical_transformed = fitted_numerical_transformer.transform(X_train[numerical_features])\n",
        "\n",
        "X_train_processed_numeric_df = pd.DataFrame(X_train_numerical_transformed, columns=numerical_features)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i, col in enumerate(numerical_features):\n",
        "    plt.subplot(1, len(numerical_features), i + 1)\n",
        "    sns.boxplot(y=X_train_processed_numeric_df[col], palette='plasma')\n",
        "    plt.title(f'{col} (After Capping & Scaling)')\n",
        "    plt.ylabel(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "HDTIwMVd_d5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:** These boxplots show the effect of the `OutlierCapper` and `StandardScaler`.\n",
        "\n",
        "- The extreme outliers seen in the 'Original' plots are now 'capped' (their values are limited to the 1st and 99th percentiles).\n",
        "- The features are also standardized (mean=0, unit variance), making their scales comparable.\n",
        "This demonstrates how our pipeline effectively handles outliers and prepares numerical features."
      ],
      "metadata": {
        "id": "ccZ-hJ0qLH0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 8: Conclusion and Next Steps\n",
        "\n",
        "This notebook has successfully demonstrated a comprehensive data cleaning and preprocessing workflow for the Titanic dataset, adhering to the provided guide and using robust sklearn pipelines.\n",
        "\n",
        "**What You've Learned and Demonstrated:**\n",
        "\n",
        "*   **Data Loading and Initial Exploration:** Understanding dataset structure, data types, and identifying missing values.\n",
        "*   **Handling Missing Values:** Using median and mode imputation strategies within a pipeline.\n",
        "*   **Categorical Feature Encoding:** Applying One-Hot Encoding for categorical-to-numerical conversion.\n",
        "*   **Feature Scaling:** Standardizing numerical features to ensure consistent scales.\n",
        "*   **Outlier Handling:** Implementing a custom `OutlierCapper` within the pipeline to mitigate the impact of extreme values.\n",
        "*   **Robust Workflow:** Building and utilizing `sklearn` Pipelines and `ColumnTransformer` for a clean, reproducible, and data-leakage-free preprocessing process.\n",
        "*   **Model Integration:** Demonstrating how preprocessing integrates seamlessly with a machine learning model.\n",
        "\n",
        "**Preparing for Submission (GitHub Repository):**\n",
        "\n",
        "To submit your task, you need to create a GitHub repository and include your work there.\n",
        "\n",
        "**Create a New GitHub Repository:**\n",
        "\n",
        "1.  Go to GitHub.com and log in.\n",
        "2.  Click the \"New\" repository button.\n",
        "3.  Give it a descriptive name (e.g., `ElevateLabs_AIML_Task1_DataPreprocessing`).\n",
        "4.  Set it to \"Public\".\n",
        "5.  Check \"Add a README file\" and \"Add .gitignore\" (select Python).\n",
        "6.  Click \"Create repository\".\n",
        "\n",
        "**Save Your Colab Notebook to GitHub:**\n",
        "\n",
        "1.  In this Google Colab notebook, go to File > Save a copy in GitHub.\n",
        "2.  Select your newly created repository from the dropdown.\n",
        "3.  Choose the `main` branch.\n",
        "4.  Click \"OK\".\n",
        "\n",
        "**Create a Comprehensive README.md File:**\n",
        "\n",
        "1.  Go to your GitHub repository.\n",
        "2.  Click on the `README.md` file and then the \"Edit\" (pencil) icon.\n",
        "3.  Populate the `README.md` with the following structure and content:"
      ],
      "metadata": {
        "id": "Jds7bi-cLKS6"
      }
    }
  ]
}